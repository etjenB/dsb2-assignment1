{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4921c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1239 unique RICs from filenames\n",
      "Saved 1177 matched RICs to c:\\Users\\Bojan\\Documents\\Master\\CSS\\3_Semester\\DSB_2\\Project_Fin\\dsb2-assignment1\\matched_rics.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# paths\n",
    "BASE_DIR = os.getcwd()\n",
    "FILES_DIR = os.path.join(BASE_DIR, \"Files\")\n",
    "\n",
    "IDENTIFIER_PATH = os.path.join(BASE_DIR, \"identifier_match.xlsx\")\n",
    "TRANSACTIONS_PATH = os.path.join(BASE_DIR, \"overview_transactions.xlsx\")\n",
    "\n",
    "OUTPUT_PATH = os.path.join(BASE_DIR, \"matched_rics.csv\")\n",
    "\n",
    "# load excel flies\n",
    "identifier_df = pd.read_excel(IDENTIFIER_PATH)\n",
    "transactions_df = pd.read_excel(TRANSACTIONS_PATH)\n",
    "\n",
    "# normalize column names\n",
    "identifier_df.columns = identifier_df.columns.str.strip()\n",
    "transactions_df.columns = transactions_df.columns.str.strip()\n",
    "\n",
    "# extract unique RICs from filenames\n",
    "ric_pattern = re.compile(r\"-([A-Z0-9\\.^]+)-\\d+-Brief\\.txt$\")\n",
    "\n",
    "rics = set()\n",
    "\n",
    "for fname in os.listdir(FILES_DIR):\n",
    "    match = ric_pattern.search(fname)\n",
    "    if match:\n",
    "        rics.add(match.group(1))\n",
    "\n",
    "print(f\"Extracted {len(rics)} unique RICs from filenames\")\n",
    "\n",
    "# prepare result rows\n",
    "rows = []\n",
    "\n",
    "for ric in sorted(rics):\n",
    "\n",
    "    # match in identifier_match.xlsx\n",
    "    id_match = identifier_df[identifier_df[\"RIC\"] == ric]\n",
    "    if id_match.empty:\n",
    "        continue\n",
    "\n",
    "    # match in overview_transactions.xlsx (RIC1 only)\n",
    "    tx_match = transactions_df[transactions_df[\"RIC1\"] == ric]\n",
    "    if tx_match.empty:\n",
    "        continue\n",
    "\n",
    "    # take first match\n",
    "    id_row = id_match.iloc[0]\n",
    "    tx_row = tx_match.iloc[0]\n",
    "\n",
    "    rows.append({\n",
    "        \"RIC\": ric,\n",
    "        \"ISIN\": id_row.get(\"ISIN\"),\n",
    "        \"CUSIP\": id_row.get(\"CUSIP\"),\n",
    "        \"AcquirorName\": tx_row.get(\"Acquiror Name\"),\n",
    "        \"TargetName\": tx_row.get(\"Target Name\"),\n",
    "        \"AnnounceDate\": tx_row.get(\"Announce Date\")\n",
    "    })\n",
    "\n",
    "# create output CSV\n",
    "result_df = pd.DataFrame(rows)\n",
    "\n",
    "result_df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"Saved {len(result_df)} matched RICs to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97009fe8",
   "metadata": {},
   "source": [
    "Getting stock prices for all companies from Compustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a8e1f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRDS recommends setting up a .pgpass file.\n",
      "pgpass file created at C:\\Users\\Bojan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\postgresql\\pgpass.conf\n",
      "Created .pgpass file successfully.\n",
      "You can create this file yourself at any time with the create_pgpass_file() function.\n",
      "Loading library list...\n",
      "Done\n",
      "Mapped rows: 1152 / 1228\n",
      "Unmapped examples:\n",
      "             RIC      CUSIP          ISIN\n",
      "2        2353.TW        NaN  TW0002353000\n",
      "3         4578.T        NaN  JP3188220002\n",
      "4      600938.SS        NaN  CNE100005980\n",
      "5         6753.T        NaN  JP3359600008\n",
      "6         6857.T        NaN  JP3122400009\n",
      "7        9961.HK        NaN  KYG9066F1019\n",
      "14        ABDN.L        NaN  GB00BF8Q6K64\n",
      "24     ACV.N^K06  013068101  US0130681010\n",
      "40    AFRE.L^H15        NaN  GB00B0672758\n",
      "64  ALTEO.PK^D08  020039707  US0200397075\n",
      "Unique events with gvkey/iid: 1152 / 1228\n",
      "[50/1152] chunk_rows=133,810 chunk_time=58.07s | elapsed=58.1s | ETA~1280.6s\n",
      "[100/1152] chunk_rows=140,628 chunk_time=18.27s | elapsed=76.4s | ETA~804.1s\n",
      "[150/1152] chunk_rows=189,216 chunk_time=22.42s | elapsed=98.9s | ETA~660.7s\n",
      "[200/1152] chunk_rows=253,191 chunk_time=28.03s | elapsed=127.1s | ETA~604.8s\n",
      "[250/1152] chunk_rows=166,334 chunk_time=15.88s | elapsed=143.0s | ETA~515.9s\n",
      "[300/1152] chunk_rows=169,550 chunk_time=18.74s | elapsed=161.8s | ETA~459.4s\n",
      "[350/1152] chunk_rows=170,354 chunk_time=16.39s | elapsed=178.2s | ETA~408.4s\n",
      "[400/1152] chunk_rows=129,922 chunk_time=12.62s | elapsed=190.9s | ETA~358.8s\n",
      "[450/1152] chunk_rows=195,191 chunk_time=198.10s | elapsed=389.0s | ETA~606.9s\n",
      "[500/1152] chunk_rows=163,799 chunk_time=143.32s | elapsed=532.4s | ETA~694.3s\n",
      "[550/1152] chunk_rows=216,927 chunk_time=22.00s | elapsed=554.5s | ETA~606.9s\n",
      "[600/1152] chunk_rows=182,736 chunk_time=166.82s | elapsed=721.4s | ETA~663.7s\n",
      "[650/1152] chunk_rows=176,268 chunk_time=170.94s | elapsed=892.4s | ETA~689.2s\n",
      "[700/1152] chunk_rows=120,476 chunk_time=106.13s | elapsed=998.6s | ETA~644.8s\n",
      "[750/1152] chunk_rows=179,789 chunk_time=173.66s | elapsed=1172.3s | ETA~628.4s\n",
      "[800/1152] chunk_rows=197,273 chunk_time=175.91s | elapsed=1348.3s | ETA~593.2s\n",
      "[850/1152] chunk_rows=145,621 chunk_time=32.39s | elapsed=1380.7s | ETA~490.6s\n",
      "[900/1152] chunk_rows=187,915 chunk_time=18.67s | elapsed=1399.5s | ETA~391.8s\n",
      "[950/1152] chunk_rows=155,935 chunk_time=16.53s | elapsed=1416.0s | ETA~301.1s\n",
      "[1000/1152] chunk_rows=161,484 chunk_time=14.58s | elapsed=1430.7s | ETA~217.5s\n",
      "[1050/1152] chunk_rows=157,407 chunk_time=21.68s | elapsed=1452.4s | ETA~141.1s\n",
      "[1100/1152] chunk_rows=187,442 chunk_time=27.77s | elapsed=1480.2s | ETA~70.0s\n",
      "[1150/1152] chunk_rows=119,962 chunk_time=10.38s | elapsed=1490.7s | ETA~2.6s\n",
      "[1152/1152] chunk_rows=1,273 chunk_time=1.45s | elapsed=1492.1s | ETA~0.0s\n",
      "Total price rows pulled: 3902503\n",
      "Saved: panel_prices_factors.csv | rows: 212740 | events: 941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bojan\\AppData\\Local\\Temp\\ipykernel_3884\\1698928379.py:172: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(slice_trading_window)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: panel_event_window_prices_factors.csv\n",
      "Rows: 10273 | Events: 941\n",
      "Saved: compustat_event_window_prices.csv\n",
      "Rows: 10273\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wrds\n",
    "import time\n",
    "\n",
    "# load matched_rics.csv\n",
    "events = pd.read_csv(\"matched_rics.csv\", parse_dates=[\"AnnounceDate\"])\n",
    "events[\"CUSIP\"] = events[\"CUSIP\"].astype(str).replace({\"nan\": np.nan, \"None\": np.nan})\n",
    "events[\"ISIN\"] = events[\"ISIN\"].astype(str).replace({\"nan\": np.nan, \"None\": np.nan})\n",
    "\n",
    "# connect to wrds\n",
    "db = wrds.Connection()\n",
    "\n",
    "# 1 build identifier -> gvkey/iid map CUSIP preferred else ISIN\n",
    "sec = db.raw_sql(\"\"\"\n",
    "    SELECT gvkey, iid, cusip, isin\n",
    "    FROM comp.security\n",
    "\"\"\")\n",
    "\n",
    "# normalize\n",
    "sec[\"cusip\"] = sec[\"cusip\"].astype(str).str.strip().replace({\"nan\": np.nan})\n",
    "sec[\"isin\"]  = sec[\"isin\"].astype(str).str.strip().replace({\"nan\": np.nan})\n",
    "\n",
    "events_map = events.copy()\n",
    "events_map[\"CUSIP\"] = events_map[\"CUSIP\"].astype(str).str.strip().replace({\"nan\": np.nan, \"None\": np.nan})\n",
    "events_map[\"ISIN\"]  = events_map[\"ISIN\"].astype(str).str.strip().replace({\"nan\": np.nan, \"None\": np.nan})\n",
    "\n",
    "# two mapping tables, then stack with priority\n",
    "cusip_map = (\n",
    "    sec.dropna(subset=[\"cusip\"])\n",
    "       .rename(columns={\"cusip\": \"id\"})\n",
    "       .assign(id_type=\"CUSIP\")[[\"id_type\", \"id\", \"gvkey\", \"iid\"]]\n",
    ")\n",
    "\n",
    "isin_map = (\n",
    "    sec.dropna(subset=[\"isin\"])\n",
    "       .rename(columns={\"isin\": \"id\"})\n",
    "       .assign(id_type=\"ISIN\")[[\"id_type\", \"id\", \"gvkey\", \"iid\"]]\n",
    ")\n",
    "\n",
    "id_map = pd.concat([cusip_map, isin_map], ignore_index=True)\n",
    "\n",
    "# create one \"preferred id\" per event: CUSIP if present else ISIN\n",
    "events_map[\"id_type\"] = np.where(events_map[\"CUSIP\"].notna() & (events_map[\"CUSIP\"] != \"\"), \"CUSIP\", \"ISIN\")\n",
    "events_map[\"id\"] = np.where(events_map[\"id_type\"] == \"CUSIP\", events_map[\"CUSIP\"], events_map[\"ISIN\"])\n",
    "\n",
    "mapped = events_map.merge(\n",
    "    id_map,\n",
    "    on=[\"id_type\", \"id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"Mapped rows:\", mapped[\"gvkey\"].notna().sum(), \"/\", len(mapped))\n",
    "print(\"Unmapped examples:\")\n",
    "print(mapped.loc[mapped[\"gvkey\"].isna(), [\"RIC\", \"CUSIP\", \"ISIN\"]].head(10))\n",
    "\n",
    "# 2 oull daily prices : efficient + progress\n",
    "mapped_ok = mapped.dropna(subset=[\"gvkey\", \"iid\"]).copy()\n",
    "\n",
    "# build (gvkey,iid) date ranges to query\n",
    "mapped_ok[\"start\"] = mapped_ok[\"AnnounceDate\"] - pd.Timedelta(days=300)  # estimation buffer\n",
    "mapped_ok[\"end\"]   = mapped_ok[\"AnnounceDate\"] + pd.Timedelta(days=30)   # event + a bit after\n",
    "\n",
    "pairs = mapped_ok[[\"gvkey\", \"iid\", \"start\", \"end\", \"RIC\"]].copy()\n",
    "\n",
    "print(f\"Unique events with gvkey/iid: {len(pairs)} / {len(mapped)}\")\n",
    "\n",
    "# wrds postgres has a limit on query length\n",
    "#query in batches and print progress\n",
    "CHUNK_SIZE = 50\n",
    "\n",
    "all_prices = []\n",
    "t0 = time.time()\n",
    "\n",
    "# helper to format values safely for SQL IN\n",
    "def sql_list(vals):\n",
    "    return \",\".join([f\"'{v}'\" for v in vals])\n",
    "\n",
    "for i in range(0, len(pairs), CHUNK_SIZE):\n",
    "    chunk = pairs.iloc[i:i+CHUNK_SIZE].copy()\n",
    "\n",
    "    # use the min/max dates for the chunk then filter in pandas by event window\n",
    "    min_date = chunk[\"start\"].min().strftime(\"%Y-%m-%d\")\n",
    "    max_date = chunk[\"end\"].max().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # build tuple list for (gvkey,iid)\n",
    "    tuple_list = \",\".join([f\"('{g}','{iid}')\" for g, iid in zip(chunk[\"gvkey\"], chunk[\"iid\"])])\n",
    "\n",
    "    query = f\"\"\"\n",
    "        SELECT gvkey, iid, datadate, prccd\n",
    "        FROM comp.secd\n",
    "        WHERE (gvkey, iid) IN ({tuple_list})\n",
    "          AND datadate BETWEEN '{min_date}' AND '{max_date}'\n",
    "          AND prccd IS NOT NULL\n",
    "    \"\"\"\n",
    "\n",
    "    q_start = time.time()\n",
    "    prices_chunk = db.raw_sql(query)\n",
    "    q_end = time.time()\n",
    "\n",
    "    prices_chunk[\"datadate\"] = pd.to_datetime(prices_chunk[\"datadate\"])\n",
    "    all_prices.append(prices_chunk)\n",
    "\n",
    "    done = min(i + CHUNK_SIZE, len(pairs))\n",
    "    elapsed = time.time() - t0\n",
    "    avg_per_event = elapsed / done\n",
    "    eta = avg_per_event * (len(pairs) - done)\n",
    "\n",
    "    print(\n",
    "        f\"[{done}/{len(pairs)}] chunk_rows={len(prices_chunk):,} \"\n",
    "        f\"chunk_time={q_end-q_start:.2f}s | elapsed={elapsed:.1f}s | ETA~{eta:.1f}s\"\n",
    "    )\n",
    "\n",
    "prices = pd.concat(all_prices, ignore_index=True) if all_prices else pd.DataFrame()\n",
    "print(\"Total price rows pulled:\", len(prices))\n",
    "\n",
    "# merge and filter per event calendar window precisely 30 day buffer\n",
    "merged = mapped_ok.merge(prices, on=[\"gvkey\", \"iid\"], how=\"left\")\n",
    "\n",
    "wide = merged[\n",
    "    (merged[\"datadate\"] >= merged[\"start\"]) &\n",
    "    (merged[\"datadate\"] <= merged[\"end\"])\n",
    "].copy()\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 1 compute returns\n",
    "wide = wide.sort_values([\"RIC\", \"datadate\"])\n",
    "wide[\"ret\"] = wide.groupby(\"RIC\")[\"prccd\"].pct_change()\n",
    "\n",
    "# 2 pull ff3 daily factors from wrds\n",
    "try:\n",
    "    ff3 = db.raw_sql(\"\"\"\n",
    "        SELECT date, mktrf, smb, hml, rf\n",
    "        FROM ff.factors_daily\n",
    "    \"\"\")\n",
    "except Exception:\n",
    "    ff3 = db.raw_sql(\"\"\"\n",
    "        SELECT date, mktrf, smb, hml, rf\n",
    "        FROM ff.factors_daily_3\n",
    "    \"\"\")\n",
    "\n",
    "ff3[\"date\"] = pd.to_datetime(ff3[\"date\"])\n",
    "\n",
    "# convert % -> decimals if needed\n",
    "if ff3[\"rf\"].abs().median() > 0.001:\n",
    "    for c in [\"mktrf\", \"smb\", \"hml\", \"rf\"]:\n",
    "        ff3[c] = ff3[c] / 100.0\n",
    "\n",
    "# 3 merge factors into your panel\n",
    "panel = wide.merge(ff3, left_on=\"datadate\", right_on=\"date\", how=\"inner\").drop(columns=[\"date\"])\n",
    "\n",
    "# 4 convenience columns\n",
    "panel[\"rm\"] = panel[\"mktrf\"] + panel[\"rf\"]          # market return\n",
    "panel[\"ri_minus_rf\"] = panel[\"ret\"] - panel[\"rf\"]   # excess stock return\n",
    "panel[\"rm_minus_rf\"] = panel[\"rm\"] - panel[\"rf\"]    # equals mktrf if constructed this way\n",
    "\n",
    "# 5save one master dataset\n",
    "panel.to_csv(\"panel_prices_factors.csv\", index=False)\n",
    "print(\"Saved: panel_prices_factors.csv | rows:\", len(panel), \"| events:\", panel[\"RIC\"].nunique())\n",
    "\n",
    "# for each event RIC, take nearest 5 trading days before and 5 after (incl event day when possible)\n",
    "def slice_trading_window(df):\n",
    "    ad = df[\"AnnounceDate\"].iloc[0]\n",
    "    df = df.dropna(subset=[\"datadate\"]).sort_values(\"datadate\")\n",
    "    before = df[df[\"datadate\"] < ad].tail(5)\n",
    "    on_after = df[df[\"datadate\"] >= ad].head(6)\n",
    "    return pd.concat([before, on_after], axis=0)\n",
    "\n",
    "event_window = (\n",
    "    panel.groupby(\"RIC\", group_keys=False)  # use panel here\n",
    "         .apply(slice_trading_window)\n",
    "         .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "event_window.to_csv(\"panel_event_window_prices_factors.csv\", index=False)\n",
    "print(\"Saved: panel_event_window_prices_factors.csv\")\n",
    "print(\"Rows:\", len(event_window), \"| Events:\", event_window[\"RIC\"].nunique())\n",
    "\n",
    "# output\n",
    "out_cols = [\"RIC\", \"ISIN\", \"CUSIP\", \"AcquirorName\", \"TargetName\", \"AnnounceDate\",\n",
    "            \"gvkey\", \"iid\", \"datadate\", \"prccd\"]\n",
    "\n",
    "event_window[out_cols].to_csv(\"compustat_event_window_prices.csv\", index=False)\n",
    "\n",
    "print(\"Saved:\", \"compustat_event_window_prices.csv\")\n",
    "print(\"Rows:\", len(event_window))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6f011a",
   "metadata": {},
   "source": [
    "1b and 1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27413428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bojan\\AppData\\Local\\Temp\\ipykernel_3884\\3002568288.py:5: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  panel = pd.read_csv(\"panel_prices_factors.csv\", parse_dates=[\"AnnounceDate\", \"datadate\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: event_AR_CAR_mm_ff3.csv\n",
      "Events with results: 905\n",
      "Rows: 9955\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "panel = pd.read_csv(\"panel_prices_factors.csv\", parse_dates=[\"AnnounceDate\", \"datadate\"])\n",
    "\n",
    "# ensure sorted\n",
    "panel = panel.sort_values([\"RIC\", \"datadate\"]).copy()\n",
    "\n",
    "# compute tau (trading-day index relative to event day) using the event-window file later,\n",
    "# but for estimation we’ll filter by dates around AnnounceDate.\n",
    "\n",
    "def fit_models_and_ar(one_event):\n",
    "    \"\"\"\n",
    "    one_event = all rows for one (RIC, AnnounceDate) from panel_prices_factors\n",
    "    returns a dataframe of event-window rows with AR/CAR for Market Model and FF3\n",
    "    \"\"\"\n",
    "    ric = one_event[\"RIC\"].iloc[0]\n",
    "    ad = one_event[\"AnnounceDate\"].iloc[0]\n",
    "\n",
    "    # define estimation window and event window by TRADING DAYS via sorting\n",
    "    one_event = one_event.dropna(subset=[\"ret\", \"mktrf\", \"smb\", \"hml\", \"rf\"]).sort_values(\"datadate\")\n",
    "\n",
    "    # create an integer index relative to announcement date based on trading days present in data\n",
    "    # tau=0 is the first trading day with datadate >= AnnounceDate\n",
    "    idx0 = one_event.index[one_event[\"datadate\"] >= ad]\n",
    "    if len(idx0) == 0:\n",
    "        return None  # no event day trading data\n",
    "    idx0 = idx0[0]\n",
    "\n",
    "    # map each row to tau using position, not calendar days\n",
    "    pos = np.arange(len(one_event))\n",
    "    pos0 = one_event.index.get_loc(idx0)\n",
    "    one_event = one_event.reset_index(drop=True)\n",
    "    one_event[\"tau\"] = np.arange(len(one_event)) - pos0\n",
    "\n",
    "    # estimation window: [-250, -30]\n",
    "    est = one_event[(one_event[\"tau\"] >= -250) & (one_event[\"tau\"] <= -30)].copy()\n",
    "    evw = one_event[(one_event[\"tau\"] >= -5) & (one_event[\"tau\"] <= 5)].copy()\n",
    "\n",
    "    # need enough observations to fit\n",
    "    if len(est) < 60:   # rule of thumb\n",
    "        return None\n",
    "\n",
    "    # Market model: ret ~ const + rm\n",
    "    # rm = mktrf + rf (market return)\n",
    "    est[\"rm\"] = est[\"mktrf\"] + est[\"rf\"]\n",
    "    evw[\"rm\"] = evw[\"mktrf\"] + evw[\"rf\"]\n",
    "\n",
    "    X_mm = sm.add_constant(est[\"rm\"])\n",
    "    y_mm = est[\"ret\"]\n",
    "    mm = sm.OLS(y_mm, X_mm).fit()\n",
    "\n",
    "    evw[\"exp_mm\"] = mm.predict(sm.add_constant(evw[\"rm\"]))\n",
    "    evw[\"ar_mm\"] = evw[\"ret\"] - evw[\"exp_mm\"]\n",
    "    evw[\"car_mm\"] = evw[\"ar_mm\"].cumsum()\n",
    "\n",
    "    # FF3: (ret - rf) ~ const + mktrf + smb + hml\n",
    "    est[\"ri_minus_rf\"] = est[\"ret\"] - est[\"rf\"]\n",
    "    evw[\"ri_minus_rf\"] = evw[\"ret\"] - evw[\"rf\"]\n",
    "\n",
    "    X_ff = sm.add_constant(est[[\"mktrf\", \"smb\", \"hml\"]])\n",
    "    y_ff = est[\"ri_minus_rf\"]\n",
    "    ff = sm.OLS(y_ff, X_ff).fit()\n",
    "\n",
    "    evw[\"exp_ff3_excess\"] = ff.predict(sm.add_constant(evw[[\"mktrf\", \"smb\", \"hml\"]]))\n",
    "    evw[\"ar_ff3\"] = evw[\"ri_minus_rf\"] - evw[\"exp_ff3_excess\"]\n",
    "    evw[\"car_ff3\"] = evw[\"ar_ff3\"].cumsum()\n",
    "\n",
    "    # keep identifiers + outputs\n",
    "    keep_cols = [\n",
    "        \"RIC\",\"AcquirorName\",\"TargetName\",\"AnnounceDate\",\"datadate\",\"tau\",\n",
    "        \"prccd\",\"ret\",\n",
    "        \"ar_mm\",\"car_mm\",\n",
    "        \"ar_ff3\",\"car_ff3\"\n",
    "    ]\n",
    "    return evw[keep_cols]\n",
    "\n",
    "# apply per event (RIC+AnnounceDate)\n",
    "out = []\n",
    "for (ric, ad), g in panel.groupby([\"RIC\", \"AnnounceDate\"]):\n",
    "    res = fit_models_and_ar(g)\n",
    "    if res is not None:\n",
    "        out.append(res)\n",
    "\n",
    "event_ar = pd.concat(out, ignore_index=True) if out else pd.DataFrame()\n",
    "event_ar.to_csv(\"event_AR_CAR_mm_ff3.csv\", index=False)\n",
    "\n",
    "print(\"Saved: event_AR_CAR_mm_ff3.csv\")\n",
    "print(\"Events with results:\", event_ar[[\"RIC\",\"AnnounceDate\"]].drop_duplicates().shape[0])\n",
    "print(\"Rows:\", len(event_ar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748be76e",
   "metadata": {},
   "source": [
    "a## Event Date, Event Window, and Estimation Window\n",
    "\n",
    "To analyze stock price reactions to merger and acquisition (M&A) announcements, an event-study framework is applied. This requires specifying an event date, an event window, and an estimation window in a transparent and theoretically motivated way.\n",
    "\n",
    "### Event Date\n",
    "The event date is defined as the **public announcement date of the M&A transaction**, as reported in the Reuters transaction database. This date represents the moment when new, value-relevant information becomes available to the market. Under the semi-strong form of the Efficient Market Hypothesis, stock prices are expected to adjust rapidly once such information is publicly disclosed, making the announcement date the natural focal point for measuring abnormal returns.\n",
    "\n",
    "### Event Window\n",
    "The event window is set to **[-5, +5] trading days** around the announcement date.\n",
    "\n",
    "This window is chosen to capture not only the immediate market reaction on the announcement day, but also potential dynamics surrounding the event:\n",
    "- The pre-announcement period \\([-5, -1]\\) allows for the possibility of **information leakage**, rumors, or anticipatory trading prior to the official announcement.\n",
    "- The post-announcement period \\([+1, +5]\\) accounts for **delayed market reactions**, gradual information processing, and potential effects of thin trading or liquidity constraints.\n",
    "\n",
    "Using a symmetric window around the event date is standard practice in the event-study literature and provides a balanced view of short-term market responses.\n",
    "\n",
    "### Estimation Window\n",
    "The estimation window used to estimate normal (expected) returns is defined as **[-250, -30] trading days** relative to the announcement date.\n",
    "\n",
    "This window is sufficiently long to obtain reliable estimates of risk parameters (e.g., betas in the market model and the Fama–French three-factor model), while being far enough from the event itself to avoid contamination from event-related information. The exclusion of the period immediately preceding the event ensures that parameter estimates are not influenced by anticipation effects or abnormal trading behavior related to the upcoming announcement.\n",
    "\n",
    "Overall, the chosen event date and windows follow established conventions in empirical finance and provide a robust basis for measuring abnormal and cumulative abnormal returns associated with M&A announcements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
